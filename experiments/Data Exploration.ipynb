{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to use `.item()` as per https://stackoverflow.com/questions/24565916/why-is-numpy-shape-empty (and also Frank's message)\n",
    "\n",
    "Also note that `mat1` contains only protons, and `mat2` contains only iron nuclei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat1 = np.load('../data/sim_12360_00.npy').item()\n",
    "mat2 = np.load('../data/sim_12362_00.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Charges', 'Energy', 'File_info', 'dir_reco', 'core_MC', 'Gain', 'core_reco', 'Position', 'dir_MC', 'Fit_status', 'Composition'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each index in each array corresponds to one cosmic ray.\n",
    "* Charges: dictionaries, where each dictionary has about (but not exactly!) 6-8 values. Each key is a sensor and each value is the value that sensor gathered. Presumably the sensors that did not recieve significant signal are not included.\n",
    "* Energy: one value per event. Unclear what that value is.\n",
    "* File_info: some information about the file that the event was drawn from \n",
    "* Gain: a list with one item-- a dictionary from sensor to whether it is a high-gain or low-gain sensor. \n",
    "* Position: the position of each sensor. Consists of an x-position array and a y-position array, but it is unclear what order they are in. \n",
    "* Fit_status: mostly `'OK'` with some `'InsufficientHits'`. Unclear where that comes into play.\n",
    "* Composition: a string representing the composition of each ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mat1['dir_MC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up dir_MC and core_MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(li):\n",
    "    new_list = []\n",
    "    for i in range(len(li)//2):\n",
    "        new_list.append((li[2*i], li[2*i+1]))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = (mat1, mat2)\n",
    "keys = ('dir_MC', 'core_MC', 'dir_reco', 'core_reco')\n",
    "for matrix in matrices:\n",
    "    for key in keys:\n",
    "        matrix[key] = cleanup(matrix[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16531"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat1['dir_reco'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the x/y positions correlate to some real-world data? Is it possible to, for example, normalize to all positive?\n",
    " * `d['Position']` is given in meters from the center of the detector. You can translate them so long as you know you're moving the detector center.\n",
    "* What does `mat1['File_info']` contain? My instinct is some information about the original simulation files that this was drawn from but perhaps not.\n",
    " * This is correct.\n",
    "* Is it the case that sensors that did not see any signal are not included in `'Charges'`?\n",
    " * `d['Charges']` does only contain triggered DOMs. This was actually a major motivation for the dictionary format, so we don't have to store all those zeros.\n",
    "* What is stored under `Energy`?\n",
    " * `d['Energy']` represents the true energy of the initial cosmic-ray that caused the shower. It's given in giga-electronvolts (GeV), but we typically look at it on a log-scale, so the simulation runs from 5-8 in log10(E/GeV).\n",
    "* Is the `position` array `[x_dict, y_dict]` or `[y_dict, x_dict]`?\n",
    " * Frank didn't explicitly answer this one but it seems like x,y.\n",
    "* Do we have access to the actual/calculated initial conditions information (angle/center) or is that something we need to calculate? If the latter, is there a script somewhere?\n",
    " * `d['Energy']` is one of those quantities. We can also pull initial direction, a variety of reconstructed directions, and potentially a reconstructed energy value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a gain-differentiated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the sensors that are high/low gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgain = list(filter(lambda x: mat1['Gain'][0][x] == 'High', mat1['Position'][0].keys()))\n",
    "lgain = list(filter(lambda x: mat1['Gain'][0][x] == 'Low', mat1['Position'][0].keys()))\n",
    "all_sensors = mat1['Gain'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a dictionary that goes from name -> column index in the matrix we're about to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a name->index dict\n",
    "def get_index_dict(sensors, direction_data=False):\n",
    "    name_index_dict = {}\n",
    "    for i in range(len(hgain)):\n",
    "        name_index_dict[hgain[i]] = i\n",
    "    if direction_data:\n",
    "        name_index_dict['zenith'] = i+1\n",
    "        name_index_dict['azimuth'] = i+2\n",
    "    return name_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgain_indices = get_index_dict(hgain)\n",
    "lgain_indices = get_index_dict(lgain)\n",
    "all_indices = get_index_dict(all_sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the matrix by setting up an empty one and populating it with the values according to the indices we created above. This is pretty slow atm but I'm sure there are ways to make it more efficient. Perhaps we don't need to make it dense in order to pop it into numpy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_matrix(proton_data, iron_data, feature_dict, direction=False):\n",
    "    event_matrix = np.zeros((len(proton_data['Composition']) + len(iron_data['Composition']), len(feature_dict) + 1))\n",
    "    for i in range(len(proton_data['Charges'])):\n",
    "        #insert sensor data\n",
    "        event = proton_data['Charges'][i]\n",
    "        for sensor in proton_data['Charges'][i].keys():\n",
    "            try:\n",
    "                event_matrix[i, feature_dict[sensor]] = 0 if math.isnan(event[sensor]) else event[sensor]\n",
    "            except KeyError:\n",
    "                continue\n",
    "        #insert direction data\n",
    "        if direction:\n",
    "            event_matrix[i, feature_dict['zenith']] = proton_data['dir_MC'][i][0]\n",
    "            event_matrix[i, feature_dict['azimuth']] = proton_data['dir_MC'][i][1]\n",
    "        event_matrix[i, -1] = 1\n",
    "    \n",
    "    for i in range(len(iron_data['Charges'])):\n",
    "        #insert sensor data\n",
    "        event = iron_data['Charges'][i]\n",
    "        for sensor in iron_data['Charges'][i].keys():\n",
    "            try:\n",
    "                event_matrix[len(proton_data['Charges']) + i, feature_dict[sensor]] = 0 if math.isnan(event[sensor]) else event[sensor]\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        #insert direction data\n",
    "        if direction:\n",
    "            event_matrix[i, feature_dict['zenith']] = iron_data['dir_MC'][i][0]\n",
    "            event_matrix[i, feature_dict['azimuth']] = iron_data['dir_MC'][i][1]    \n",
    "        event_matrix[len(proton_data['Charges']) + i, -1] = 0\n",
    "        \n",
    "    return event_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgain_events = gen_matrix(mat1, mat2, hgain_indices)\n",
    "lgain_events = gen_matrix(mat1, mat2, lgain_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31620, 163)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgain_events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split high-gain into training/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(hgain_events)\n",
    "train_size = int(hgain_events.shape[0] * .9)\n",
    "\n",
    "trainset = hgain_events[:train_size]\n",
    "testset = hgain_events[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[trainset[:,-1]==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('small_train.npy', trainset)\n",
    "np.save('small_test.npy', testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test set with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_events = gen_matrix(mat1, mat2, all_indices)\n",
    "np.random.shuffle(all_events)\n",
    "train_size = int(all_events.shape[0] * .9)\n",
    "\n",
    "trainset = all_events[:train_size]\n",
    "testset = all_events[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[trainset[:,-1]==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('small_train_high_low.npy', trainset)\n",
    "np.save('small_test_high_low.npy', testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add azimuth, zenith features to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_indices_dir = get_index_dict(all_sensors, direction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_direction = gen_matrix(mat1, mat2, all_indices_dir, direction=True)\n",
    "np.random.shuffle(all_events_direction)\n",
    "train_size = int(all_events_direction.shape[0] * .9)\n",
    "\n",
    "trainset = all_events_direction[:train_size]\n",
    "testset = all_events_direction[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31620, 165)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_events_direction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('direction_train.npy',trainset)\n",
    "np.save('direction_test.npy', testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mat1['Charges']) + len(mat2['Charges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring width of charge readings\n",
    "In this section, I'm trying to figure out how widely the charge readings go. How easy will it be to trim the number of input features to those around the centroid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extreme_pos(charge_dict, positions, big=True):\n",
    "    l = list(map(lambda x:positions[x], charge_dict.keys()))\n",
    "    if big:\n",
    "        return max(l)\n",
    "    return min(l)\n",
    "\n",
    "def get_bounding_box(charge_dict, positions):\n",
    "    min_x = get_extreme_pos(charge_dict, positions[0], big=False)\n",
    "    max_x = get_extreme_pos(charge_dict, positions[0], big=True)\n",
    "    min_y = get_extreme_pos(charge_dict, positions[1], big=False)\n",
    "    max_y = get_extreme_pos(charge_dict, positions[1], big=True)\n",
    "    return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "def get_dimensions(charge_dict, positions):\n",
    "    min_x, max_x, min_y, max_y = get_bounding_box(charge_dict, positions)\n",
    "    return max_x - min_x, max_y - min_y\n",
    "\n",
    "dimensions = [get_dimensions(event, mat1['Position']) for event in mat1['Charges']]\n",
    "\n",
    "widest = max(dimensions, key=lambda x: x[0])\n",
    "tallest = max(dimensions, key=lambda x: x[1])\n",
    "print(widest, tallest)\n",
    "# width, height = get_dimensions(mat1['Charges'][0], mat1['Position'])\n",
    "# print('width: {} height: {}'.format(width, height))\n",
    "# print('X: ({},{}), Y: ({},{})'.format(min_x,max_x,min_y,max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = max(mat1['Position'][0].values()) - min(mat1['Position'][0].values())\n",
    "height = max(mat1['Position'][1].values()) - min(mat1['Position'][1].values())\n",
    "print('{} {}'.format(width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can't willy-nilly trim to the max width, since at least one reading spreads across the entire height or width. \n",
    "\n",
    "The next thing to try is some kind of center-of-mass trimming. First, I want to find out the average width and height so we can see how much trimming seems appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [x[0] for x in dimensions]\n",
    "heights = [x[1] for x in dimensions]\n",
    "print('width: {} height: {}'.format(np.median(widths), np.median(heights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_com(charge_dict, positions):\n",
    "    xs = [positions[0][x] for x in charge_dict.keys()]\n",
    "    ys = [positions[1][y] for y in charge_dict.keys()]\n",
    "    weights = [x for x in charge_dict.values()]\n",
    "    x = np.average(xs, weights=weights)\n",
    "    y = np.average(ys, weights=weights)\n",
    "    return(x,y)\n",
    "\n",
    "def get_positions(positions_dict_x, positions_dict_y):\n",
    "    positions = {}\n",
    "    for sensor in positions_dict_x.keys():\n",
    "        positions[sensor] = (positions_dict_x[sensor], positions_dict_y[sensor])\n",
    "    return positions\n",
    "\n",
    "def get_coms(charge_dicts, positions):\n",
    "    return [get_com(event, positions) for event in charge_dicts]\n",
    "\n",
    "def in_bounds(position, x_bound, y_bound):\n",
    "    in_bounds_x = position[0] <= x_bound[1] and position[0] >= x_bound[0]\n",
    "    in_bounds_y = position[1] <= y_bound[1] and position[1] >= y_bound[0]\n",
    "    return in_bounds_x and in_bounds_y\n",
    "\n",
    "def get_trimmed_charge_dict(charge_dict, positions, x_bound, y_bound):\n",
    "    return {key : value for key,value in charge_dict.items() if in_bounds((positions[0][key], positions[1][key]), x_bound, y_bound)}\n",
    "\n",
    "def get_trimmed_charge_dicts(charge_dicts, positions):\n",
    "    coms = get_coms(charge_dicts, positions)\n",
    "    normalized_positions = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bounding_box(mat1['Charges'][0], mat1['Position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed = get_trimmed_charge_dict(mat1['Charges'][0], mat1['Position'], (0, 160), (0, 76))\n",
    "# list(map(lambda x: (mat1['Position'][0][x],mat1['Position'][1][x]), trimmed.keys()))\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = get_positions(mat1['Position'][0], mat1['Position'][1])\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targs = get_coms(mat1['Charges'],mat1['Position'])\n",
    "\n",
    "#pos_a: \n",
    "def get_distance(pos_a, pos_b):\n",
    "    return np.sqrt(np.sum(np.power(np.array(pos_a)-np.array(pos_b), 2)))\n",
    "\n",
    "def get_distances(target, all_positions):\n",
    "    return {sensor: get_distance(target, all_positions[sensor]) for sensor in all_positions.keys()}\n",
    "\n",
    "def get_nearest_neighbor(target, all_positions):\n",
    "    distances = {sensor: get_distance(target, all_positions[sensor]) for sensor in all_positions.keys()}\n",
    "    min_sensor = 0\n",
    "    min_value = 99999999999999999999\n",
    "    for sensor in distances.keys():\n",
    "        if distances[sensor] < min_value:\n",
    "            min_sensor = sensor\n",
    "            min_value = distances[sensor]\n",
    "    return min_sensor\n",
    "\n",
    "# d = get_distances(targ, positions)\n",
    "# min_sensor = 0\n",
    "# min_value = 999999999999999999\n",
    "# for sensor in d.keys():\n",
    "#     if d[sensor] < min_value:\n",
    "#         min_sensor = sensor\n",
    "#         min_value = d[sensor]\n",
    "# min_sensor\n",
    "res = []\n",
    "for center in targs:\n",
    "    res.append(get_nearest_neighbor(center, positions))\n",
    "    if(len(res) % 500 == 0):\n",
    "        print(len(res) / len(targs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
